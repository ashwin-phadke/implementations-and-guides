{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Tensorflow Authors\n",
    "\n",
    "# Simple sentence encoding\n",
    "# Notes : Remember tokenizer strips punctuation so I is `i`.\n",
    "# Do necessary imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences to encode\n",
    "# The exclamation doesn't impact the encoding, check that out too.\n",
    "sentences = [\"I love my dog\", \"I love my cat\", \"you love my dog!\", \"Do you think my dog si amazing?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of Tokenizer - take top 100 words by volume and just encode those from the available dataset, this hyperparameter can be changed as per requirement.\n",
    "\n",
    "# Added OOV token to recognize words that are out of vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "\n",
    "# This method takes in the data i.e sentences and then encodes it\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOkenixer provides word index which returns dictionary of key-value pairs where key=word and value=token\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "print(word_index['my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turns into set of sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Generates a list of sentences that have been encoded into integer lists\n",
    "# Tokens replace the words - I love my dog becomes [x, y, z, w]\n",
    "print(sequences)\n",
    "\n",
    "# Test the fit , you may notice that the words that weren't encoded did not generate and were excluded from the results. When you add OOV then it includes the index for OOV\n",
    "test_data = [\"I really love my dog\", \"my dog loves my manatee\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)\n",
    "\n",
    "# Pass sequences for padding. It pads out in a form of matrix where each row represents a sentence wherein row size is the size of the largest sentence often.\n",
    "\n",
    "# Padding post adds padding after the sentence.\n",
    "\n",
    "# Use maxlen parameter to determine how long your sentence should be in terms of words. You will loose sentences if you set maxlen lower, with padding generally being pre you will loose from the beginning of the sentence, so to override this use the truncating parameter as `truncating=post`\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.1 64-bit",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}